# -*- coding: utf-8 -*-
"""Final package.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V2tsykkXVxJNTdyKt7AJovqRv78vj-e6
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import pyplot as pyplot
# %matplotlib inline
from sklearn.preprocessing import normalize
import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import Birch
from matplotlib import style
style.use('ggplot')
from sklearn.datasets import make_moons
from sklearn.datasets import make_circles
from pandas import DataFrame
from sklearn.cluster import AffinityPropagation 
from sklearn import metrics 
from itertools import cycle 
from sklearn.cluster import SpectralClustering 
from sklearn.preprocessing import StandardScaler, normalize 
from sklearn.decomposition import PCA 
from sklearn.metrics import silhouette_score 
from pprint import pprint
from csv import reader
import seaborn as sns; sns.set()  # for plot styling 
import seaborn as seabornInstance 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from random import seed
from random import randrange
from sklearn.datasets.samples_generator import make_blobs

def hierarchial(data):
  print("                                  --------------------------")
  print("                                    Hierarchial Clustering")
  print("                                   --------------------------")
  data = pd.read_csv('tae.csv')

  data_scaled = normalize(data)
  data_scaled = pd.DataFrame(data_scaled, columns=data.columns)

  plt.figure(figsize=(10, 7))  
  plt.title("Dendrograms")  
  dend = shc.dendrogram(shc.linkage(data_scaled, method='ward'))
  plt.axhline(y=3.5, color='r', linestyle='--')

  cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  
  cluster.fit_predict(data_scaled)

  plt.figure(figsize=(10, 7))  
  plt.scatter(data_scaled['Course'], data_scaled['Course instructor'], c=cluster.labels_) 
  print("--------------------------------------------------------------------------------------------------------")


def density(df):
  print("                                   ------------------------------------")
  print("                                     Density Based Spatial Clustering")
  print("                                   ------------------------------------")
  df = df.drop(columns=['Class Attribute','Semester type','Speaker Type']) 
  data = df.to_numpy()

  np.random.seed(12)
  p1 = np.random.randint(5,21,110) 
  p2 = np.random.randint(20,30,120)
  p3 = np.random.randint(8,21,90)

  data = np.array(np.concatenate([p1, p2, p3]))
  x_range = range(len(data))
  x = np.array(list(zip(x_range, data))).reshape(len(x_range), 2)

  plt.scatter(x[:,0], x[:,1])
  plt.show()

  bclust=Birch(branching_factor=100, threshold=.5).fit(x)
  print(bclust)

  labels = bclust.predict(x)

  plt.scatter(x[:,0], x[:,1], c=labels)
  plt.show()
  print("--------------------------------------------------------------------------------------------------------")


def meanshift(df):
  print("                                   -------------------------")
  print("                                     Mean Shift Clustering")
  print("                                   -------------------------")
  df = df.drop(columns=['Course','Class Attribute','Semester type','Speaker Type']) 
  X = df.to_numpy()

  colors = 10*["g","r","c","b","k"]

  class Mean_Shift:
      def __init__(self, radius=4):
          self.radius = radius

      def fit(self, data):
          centroids = {}

          for i in range(len(data)):
              centroids[i] = data[i]
        
          while True:
              new_centroids = []
              for i in centroids:
                  in_bandwidth = []
                  centroid = centroids[i]
                  for featureset in data:
                      if np.linalg.norm(featureset-centroid) < self.radius:
                          in_bandwidth.append(featureset)

                  new_centroid = np.average(in_bandwidth,axis=0)
                  new_centroids.append(tuple(new_centroid))

              uniques = sorted(list(set(new_centroids)))

              prev_centroids = dict(centroids)

              centroids = {}
              for i in range(len(uniques)):
                  centroids[i] = np.array(uniques[i])

              optimized = True

              for i in centroids:
                  if not np.array_equal(centroids[i], prev_centroids[i]):
                      optimized = False
                  if not optimized:
                      break
                
              if optimized:
                  break

          self.centroids = centroids

  clf = Mean_Shift()
  clf.fit(X)

  centroids = clf.centroids

  plt.scatter(X[:,0], X[:,1], s=150)

  for c in centroids:
      plt.scatter(centroids[c][0], centroids[c][1], color='k', marker='*', s=150)

  plt.show()
  print("--------------------------------------------------------------------------------------------------------")



def birch(df):
  print("                                    ----------------------")
  print("                                      Birch Clustering")
  print("                                    ----------------------")
  df1 = df.drop(columns=['Class Attribute','Semester type','Speaker Type','Course','Course instructor']) 
  df2 = df.drop(columns=['Class Attribute','Semester type','Speaker Type','Course','Class Size']) 
  df3 = df.drop(columns=['Class Attribute','Semester type','Speaker Type','Class Size','Course instructor'])  
  p1 = df1.to_numpy()
  p2 = df2.to_numpy()
  p3 = df3.to_numpy()


  data = np.array(np.concatenate([p1, p2, p3]))

  x_range = range(len(data))
  x = np.array(list(zip(x_range, data))).reshape(len(x_range), 2)

  plt.scatter(x[:,0], x[:,1])
  plt.show()

  bclust=Birch(branching_factor=100, threshold=.5).fit(x)
  print(bclust)

  labels = bclust.predict(x)

  plt.scatter(x[:,0], x[:,1], c=labels)
  plt.show()


def affinity_propogation(df):
  print("                                   -----------------------------------")
  print("                                     Affinity Propogation Clustering")
  print("                                   -----------------------------------")
  centers = [[1, 1], [-1, -1], [1, -1], [-1, -1]] 

  df = df.drop(columns=['Class Attribute','Speaker Type','Course instructor','Semester type']) 
  X = df.to_numpy()

  af = AffinityPropagation(preference =-50).fit(X) 
  cluster_centers_indices = af.cluster_centers_indices_ 
  labels = af.labels_ 

  n_clusters_ = len(cluster_centers_indices) 

  plt.close('all') 
  plt.figure(1) 
  plt.clf() 

  colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk') 

  for k, col in zip(range(n_clusters_), colors): 
	  class_members = labels == k 
	  cluster_center = X[cluster_centers_indices[k]] 
	  plt.plot(X[class_members, 0], X[class_members, 1], col + '.') 
	  plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor = col, markeredgecolor ='k',markersize = 8) 
  
  for x in X[class_members]:
    plt.plot([cluster_center[0], x[0]],[cluster_center[1], x[1]], col) 

  plt.title('Estimated number of clusters: % d' % n_clusters_) 
  plt.show() 
  print("--------------------------------------------------------------------------------------------------------")

def spectral(X):
  print("                                   -----------------------")
  print("                                     Spectral Clustering")
  print("                                   -----------------------")
  X.fillna(method ='ffill', inplace = True) 
  scaler = StandardScaler() 
  X_scaled = scaler.fit_transform(X) 
  X_normalized = normalize(X_scaled) 
  X_normalized = pd.DataFrame(X_normalized) 
  pca = PCA(n_components = 2) 
  X_principal = pca.fit_transform(X_normalized) 
  X_principal = pd.DataFrame(X_principal) 
  X_principal.columns = ['P1', 'P2'] 
#1st type  
  spectral_model_rbf = SpectralClustering(n_clusters = 2, affinity ='rbf') 
  labels_rbf = spectral_model_rbf.fit_predict(X_principal) 

  colours = {} 
  colours[0] = 'b'
  colours[1] = 'y'
  cvec = [colours[label] for label in labels_rbf] 
  print("-----------------")
  print("a) Affinity=‘rbf’")
  print("-----------------")
  b = plt.scatter(X_principal['P1'], X_principal['P2'], color ='b'); 
  y = plt.scatter(X_principal['P1'], X_principal['P2'], color ='y'); 

  plt.figure(figsize =(9, 9)) 
  plt.scatter(X_principal['P1'], X_principal['P2'], c = cvec) 
  plt.legend((b, y), ('Label 0', 'Label 1')) 
  plt.show() 

#2nd type  
  spectral_model_nn = SpectralClustering(n_clusters = 2, affinity ='nearest_neighbors') 
  labels_nn = spectral_model_nn.fit_predict(X_principal) 

  colours = {} 
  colours[0] = 'b'
  colours[1] = 'y'
  cvec = [colours[label] for label in labels_rbf] 
  print('-------------------------------')
  print("b) Affinity=‘nearest_neighbors’")
  print('-------------------------------')
  b = plt.scatter(X_principal['P1'], X_principal['P2'], color ='b'); 
  y = plt.scatter(X_principal['P1'], X_principal['P2'], color ='y'); 
  
  plt.figure(figsize =(9, 9)) 
  plt.scatter(X_principal['P1'], X_principal['P2'], c = cvec) 
  plt.legend((b, y), ('Label 0', 'Label 1')) 
  plt.show() 

#comparision

  affinity = ['rbf', 'nearest-neighbours'] 
  s_scores = [] 
  s_scores.append(silhouette_score(X, labels_rbf)) 
  s_scores.append(silhouette_score(X, labels_nn)) 

  print(s_scores) 

  plt.bar(affinity, s_scores) 
  plt.xlabel('Affinity') 
  plt.ylabel('Silhouette Score') 
  plt.title('Comparison of different Clustering Models') 
  plt.show() 
  print("--------------------------------------------------------------------------------------------------------")


def kmeans(df):
  print("                                   -----------------------")
  print("                                     K-Means Clustering")
  print("                                   -----------------------")
  plt.scatter(df['Course'],df['Class Size'])
  plt.show()
  km=KMeans(n_clusters=3)
  km
  y_predicted=km.fit_predict(df[['Course','Class Size']])
  y_predicted
  df['cluster']=y_predicted
  df1=df[df.cluster==0]
  df2=df[df.cluster==1]
  df3=df[df.cluster==2]

  plt.scatter(df1.Course,df1['Class Size'],color='green')
  plt.scatter(df2.Course,df2['Class Size'],color='red')
  plt.scatter(df3.Course,df3['Class Size'],color='black')

  plt.xlabel('Course')
  plt.ylabel('Class Size')

def pair_plot(df):
  print("---------------")
  print("   Pair Plot")
  print("---------------")
  sns.set(style="ticks")
  sns.pairplot(df, hue="Speaker Type")
  print("--------------------------------------------------------------------------------------------------------")

def heat_map(df):
  print("--------------")
  print("   Heat Map")
  print("--------------")
  tc = df.corr() 
  sns.heatmap(tc, annot = True, cmap ='plasma',linecolor ='black', linewidths = 1) 
  print("--------------------------------------------------------------------------------------------------------")

def cluster_map(df):
  print("-----------------")
  print("   Cluster Map")
  print("----------------")
  sns.clustermap(df, cmap ='plasma', standard_scale = 1) 
  print("--------------------------------------------------------------------------------------------------------")

def entropy(target_col):
    elements,counts = np.unique(target_col,return_counts = True)
    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])
    return entropy

def InfoGain(data,split_attribute_name,target_name="attribute"):
    total_entropy = entropy(data[target_name])
    vals,counts= np.unique(data[split_attribute_name],return_counts=True)
    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])
    Information_Gain = total_entropy - Weighted_Entropy
    return Information_Gain

def ID3(data,originaldata,features,target_attribute_name="attribute",parent_node_class = None):
    if len(np.unique(data[target_attribute_name])) <= 1:
        return np.unique(data[target_attribute_name])[0]
    elif len(data)==0:
        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]
    elif len(features) ==0:
        return parent_node_class    
    else:
        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]
        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset
        best_feature_index = np.argmax(item_values)
        best_feature = features[best_feature_index]
        tree = {best_feature:{}}
        features = [i for i in features if i != best_feature]
        for value in np.unique(data[best_feature]):
            value = value
            sub_data = data.where(data[best_feature] == value).dropna()
            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)
            tree[best_feature][value] = subtree     
        return(tree)    
    
def predict(query,tree,default = 1):
    for key in list(query.keys()):
        if key in list(tree.keys()):
            try:
                result = tree[key][query[key]] 
            except:
                return default
            result = tree[key][query[key]]
            if isinstance(result,dict):
                return predict(query,result)
            else:
                return result

        
def train_test_split(dataset):
    training_data = dataset.iloc[:80].reset_index(drop=True)
    testing_data = dataset.iloc[80:].reset_index(drop=True)
    return training_data,testing_data

def test(data,tree):
    queries = data.iloc[:,:-1].to_dict(orient = "records")
    predicted = pd.DataFrame(columns=["predicted"]) 
    for i in range(len(data)):
        predicted.loc[i,"predicted"] = predict(queries[i],tree,1.0) 
    print('The prediction accuracy is: ',(np.sum(predicted["predicted"] == data["class"])/len(data))*100,'%')


#-------------------------------------

# CART 

def load_csv(filename):
	file = open(filename, "rt")
	lines = reader(file)
	dataset = list(lines)
	dataset.pop(0)
	return dataset

def str_column_to_float(dataset, column):
	for row in dataset:
		row[column] = float(row[column].strip())


def cross_validation_split(dataset, n_folds):
	dataset_split = list()
	dataset_copy = list(dataset)
	fold_size = int(len(dataset) / n_folds)
	for i in range(n_folds):
		fold = list()
		while len(fold) < fold_size:
			index = randrange(len(dataset_copy))
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split

def accuracy_metric(actual, predicted):
	correct = 0
	for i in range(len(actual)):
		if actual[i] == predicted[i]:
			correct += 1
	return correct / float(len(actual)) * 100.0

def evaluate_algorithm(dataset, algorithm, n_folds, *args):
	folds = cross_validation_split(dataset, n_folds)
	scores = list()
	for fold in folds:
		train_set = list(folds)
		train_set.remove(fold)
		train_set = sum(train_set, [])
		test_set = list()
		for row in fold:
			row_copy = list(row)
			test_set.append(row_copy)
			row_copy[-1] = None
		predicted = algorithm(train_set, test_set, *args)
		actual = [row[-1] for row in fold]
		accuracy = accuracy_metric(actual, predicted)
		scores.append(accuracy)
	return scores

def test_split(index, value, dataset):
	left, right = list(), list()
	for row in dataset:
		if row[index] < value:
			left.append(row)
		else:
			right.append(row)
	return left, right

def gini_index(groups, classes):
	n_instances = float(sum([len(group) for group in groups]))
	gini = 0.0
	for group in groups:
		size = float(len(group))
		if size == 0:
			continue
		score = 0.0
		for class_val in classes:
			p = [row[-1] for row in group].count(class_val) / size
			score += p * p
		gini += (1.0 - score) * (size / n_instances)
	return gini

def get_split(dataset):
	class_values = list(set(row[-1] for row in dataset))
	b_index, b_value, b_score, b_groups = 999, 999, 999, None
	for index in range(len(dataset[0])-1):
		for row in dataset:
			groups = test_split(index, row[index], dataset)
			gini = gini_index(groups, class_values)
			if gini < b_score:
				b_index, b_value, b_score, b_groups = index, row[index], gini, groups
	return {'index':b_index, 'value':b_value, 'groups':b_groups}

def to_terminal(group):
	outcomes = [row[-1] for row in group]
	return max(set(outcomes), key=outcomes.count)

def split(node, max_depth, min_size, depth):
	left, right = node['groups']
	del(node['groups'])
	if not left or not right:
		node['left'] = node['right'] = to_terminal(left + right)
		return
	if depth >= max_depth:
		node['left'], node['right'] = to_terminal(left), to_terminal(right)
		return
	if len(left) <= min_size:
		node['left'] = to_terminal(left)
	else:
		node['left'] = get_split(left)
		split(node['left'], max_depth, min_size, depth+1)
	if len(right) <= min_size:
		node['right'] = to_terminal(right)
	else:
		node['right'] = get_split(right)
		split(node['right'], max_depth, min_size, depth+1)


def build_tree(train, max_depth, min_size):
  root = get_split(train)
  split(root, max_depth, min_size, 1)
  return root


def print_tree(node, depth=0):
	if isinstance(node, dict):
		print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))
		print_tree(node['left'], depth+1)
		print_tree(node['right'], depth+1)
	else:
		print('%s[%s]' % ((depth*' ', node)))
  

def predict(node, row):
	if row[node['index']] < node['value']:
		if isinstance(node['left'], dict):
			return predict(node['left'], row)
		else:
			return node['left']
	else:
		if isinstance(node['right'], dict):
			return predict(node['right'], row)
		else:
			return node['right']


def decision_tree(train, test, max_depth, min_size):
  tree = build_tree(train, max_depth, min_size)
  print_tree(tree)
  predictions = list()
  for row in test:
    prediction = predict(tree, row)
    predictions.append(prediction)
  return(predictions)

def clustering(data):
  print("----------------------------------------------------------------------------------------------------------")

  print("Clustering Types")
  print("     1.hierarchial clustering")
  print("     2.density based clustering")
  print("     3.mean shift clustering")
  print("     4.birch clustering")
  print("     5.Affinity propogation clustering")
  print("     6.Spectral Clustering")
  print("     7.kmeans Clustering")
  print(" ")
  val = input(" Choose : ") 

  def indirect(i):
      switcher={
              '1':hierarchial,
              '2':density,
              '3':meanshift,
              '4':birch,
              '5':affinity_propogation,
              '6':spectral,
              '7':kmeans
              }
      func=switcher.get(i,lambda :'Invalid')
      return func(data)
  indirect(val)

def decisiontree(dataset):
  print("----------------------------------------------------------------------------------------------------------")
  print("Decision Trees")
  print("     1.ID3 algorithm")
  print("     2.Cart algorithm")
  print(" ")
  val = input(" Choose : ") 
  print(" ")
  if val == '1':
    dataset = pd.read_csv('tae.csv',names=['English','instructor','Course','semester','size','attribute',])
    print("                                   -----------------------")
    print("                                         ID3 algorithm")
    print("                                   -----------------------")
    print(" ")
    training_data = train_test_split(dataset)[0]
    testing_data = train_test_split(dataset)[1] 
    tree = ID3(training_data,training_data,training_data.columns[:-1])
    pprint(tree)
  elif val=='2':
    print("                                   -----------------------")
    print("                                         Cart algorithm")
    print("                                   -----------------------")
    print(" ")
    filename = 'tae.csv'
    dataset = load_csv(filename)
    for i in range(len(dataset[0])):
      str_column_to_float(dataset, i)
    n_folds = 5
    max_depth = 5
    min_size = 10
    scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)
    print(" ")
    print("Conclusions")
    print(" ")
    print('Scores....: %s' % scores)
    print('Mean Accuracy....: %.3f%%' % (sum(scores)/float(len(scores))))

def matrix_plots(df):
  print("")
  print('MATRIX PLOTS')
  print("")
  print("   1.Pair Plot")
  print("   5.Heat Map")
  print("   6.Cluster Map")
  print(" ")
  val = input(" Choose : ") 
  def indirect(i):
      switcher={
              '1':pair_plot,
              '2':heat_map,
              '3':cluster_map
              }
      func=switcher.get(i,lambda :'Invalid')
      return func(df)
  indirect(val)

def graph(dataset):



  # generate 2d classification dataset
  X, y = make_blobs(n_samples=100, centers=3, n_features=2)
  # scatter plot, dots colored by class value
  df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))
  colors = {0:'red', 1:'blue', 2:'green'}
  fig, ax = pyplot.subplots()
  grouped = df.groupby('label')
  for key, group in grouped:
    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])
  pyplot.show()





  # generate 2d classification dataset
  X, y = make_moons(n_samples=100, noise=0.1)
  # scatter plot, dots colored by class value
  df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))
  colors = {0:'red', 1:'blue'}
  fig, ax = pyplot.subplots()
  grouped = df.groupby('label')
  for key, group in grouped:
    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])
  pyplot.show()






  # generate 2d classification dataset
  X, y = make_circles(n_samples=100, noise=0.05)
  # scatter plot, dots colored by class value
  df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))
  colors = {0:'red', 1:'blue'}
  fig, ax = pyplot.subplots()
  grouped = df.groupby('label')
  for key, group in grouped:
    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])



  dataset = pd.read_csv('tae.csv')
  df=pd.read_csv('tae.csv')
  df.head()

  plt.bar(df['Course instructor'],df['Class Size']) 
  plt.title('Course instructor vs Class Size')  
  plt.xlabel('Course instructor')  
  plt.ylabel('Class Size')  
  plt.show()
  print("INFERENCE FROM BAR GRAPH::Course instructor no.15 has larger class size")

data=pd.read_csv('tae.csv')
print("----------------------------------------------------------------------------------------------------------")
print("                                  Predictive analysis of TAE dataset")
print("----------------------------------------------------------------------------------------------------------")
print(" The following are the analysis done on TAE dataset....")
print("         1.Clustering types")
print("         2.Decision Trees")
print("         3.Matrix plots")
print("         4.Graphs")
print(" ")
val = input(" Choose: ") 
print("----------------------------------------------------------------------------------------------------------")


def indirect(i):
    switcher={
            '1':clustering,
            '2':decisiontree,
            '3':matrix_plots,
            '4':graph
            }
    func=switcher.get(i,lambda :'Invalid')
    return func(data)
indirect(val)
